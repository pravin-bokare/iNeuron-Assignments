{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fcf3322",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64294806",
   "metadata": {},
   "source": [
    "The purpose of the General Linear Model (GLM) is to analyze the relationship between a dependent variable and one or more independent variables. It is a flexible framework that encompasses various statistical models, including linear regression, logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972b3d3e",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbef4ac3",
   "metadata": {},
   "source": [
    "The key assumptions of the General Linear Model include linearity (the relationship between the variables is linear), independence of observations, homoscedasticity (constant variance of errors), and normality of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f26cfee",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fb24dc",
   "metadata": {},
   "source": [
    "The key assumptions of the General Linear Model include linearity (the relationship between the variables is linear), independence of observations, homoscedasticity (constant variance of errors), and normality of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d46b6de",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37c20b2",
   "metadata": {},
   "source": [
    "A univariate GLM involves a single dependent variable, while a multivariate GLM involves multiple dependent variables. In a multivariate GLM, the relationships between the independent variables and each dependent variable are analyzed simultaneously, taking into account the potential correlations among the dependent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3d5841",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772d1635",
   "metadata": {},
   "source": [
    "Interaction effects in a GLM occur when the relationship between an independent variable and the dependent variable is influenced by another independent variable. It means that the effect of one independent variable on the dependent variable depends on the level of another independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c25316",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302ef546",
   "metadata": {},
   "source": [
    "Categorical predictors in a GLM are typically encoded using dummy variables. Each category is represented by a binary variable (0 or 1) indicating the presence or absence of that category. These dummy variables are then included as predictors in the GLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ab5a2a",
   "metadata": {},
   "source": [
    "# Q7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0ef5a7",
   "metadata": {},
   "source": [
    "The design matrix in a GLM is a matrix that contains the predictor variables, including any interactions or transformations, as columns. It is used to represent the relationships between the predictors and the dependent variable and is essential for estimating the coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1afff6",
   "metadata": {},
   "source": [
    "# Q8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25626b08",
   "metadata": {},
   "source": [
    "The significance of predictors in a GLM can be tested using hypothesis tests, such as t-tests or F-tests, to assess whether the estimated coefficients are significantly different from zero. The p-values associated with these tests indicate the statistical significance of the predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c14ccd",
   "metadata": {},
   "source": [
    "# Q9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c6bf92",
   "metadata": {},
   "source": [
    "Type I, Type II, and Type III sums of squares are different methods for partitioning the variation in the dependent variable among the predictors in a GLM. Type I sums of squares reflect the unique contribution of each predictor, but their interpretation depends on the order in which predictors are entered into the model. Type II and Type III sums of squares are more robust and do not depend on the order of predictor entry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22bd465",
   "metadata": {},
   "source": [
    "# Q10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6a3ad6",
   "metadata": {},
   "source": [
    "Deviance in a GLM is a measure of the goodness-of-fit of the model. It represents the difference between the observed data and the predicted values based on the model. A lower deviance indicates a better fit to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0d0d9d",
   "metadata": {},
   "source": [
    "# Q11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f064f5",
   "metadata": {},
   "source": [
    "Regression analysis is a statistical method used to model the relationship between a dependent variable and one or more independent variables. Its purpose is to understand how changes in the independent variables are associated with changes in the dependent variable and to make predictions or infer causal relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c75ec64",
   "metadata": {},
   "source": [
    "# Q12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d222c49a",
   "metadata": {},
   "source": [
    "Simple linear regression involves one dependent variable and one independent variable, while multiple linear regression involves one dependent variable and two or more independent variables. Simple linear regression assumes a linear relationship between the variables, while multiple linear regression allows for more complex relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce646c66",
   "metadata": {},
   "source": [
    "# Q13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185bc315",
   "metadata": {},
   "source": [
    "The R-squared value in regression, also known as the coefficient of determination, represents the proportion of the variance in the dependent variable that can be explained by the independent variables. It ranges from 0 to 1, where 0 indicates no relationship and 1 indicates a perfect fit. However, R-squared alone does not indicate the model's validity or the significance of the predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaf29c7",
   "metadata": {},
   "source": [
    "# Q14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6904c2",
   "metadata": {},
   "source": [
    "Correlation measures the strength and direction of the linear relationship between two variables, while regression aims to model the relationship between a dependent variable and one or more independent variables. Regression provides insights into how changes in the independent variables impact the dependent variable, whereas correlation does not imply causation or provide information about the directionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6396a7",
   "metadata": {},
   "source": [
    "# Q15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c14c845",
   "metadata": {},
   "source": [
    "The coefficients in regression represent the estimated change in the dependent variable associated with a one-unit change in the corresponding independent variable, assuming other variables are held constant. The intercept represents the expected value of the dependent variable when all independent variables are zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261093d1",
   "metadata": {},
   "source": [
    "# Q16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c795a0a",
   "metadata": {},
   "source": [
    "Outliers in regression analysis are extreme observations that may disproportionately influence the estimated coefficients and the overall model fit. Handling outliers can involve identifying and removing them, transforming the data, or using robust regression techniques that are less sensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c0e9e2",
   "metadata": {},
   "source": [
    "# Q17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0e9647",
   "metadata": {},
   "source": [
    "Ordinary least squares (OLS) regression is a widely used method that minimizes the sum of squared residuals to estimate the regression coefficients. Ridge regression, on the other hand, introduces a penalty term to the least squares objective function to shrink the coefficients towards zero and reduce the impact of multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03af1ff8",
   "metadata": {},
   "source": [
    "# Q18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7000f8",
   "metadata": {},
   "source": [
    "Heteroscedasticity in regression refers to the unequal variance of the errors across the range of the independent variables. It violates the assumption of homoscedasticity and can affect the accuracy of coefficient estimates and the validity of statistical tests. Techniques such as weighted least squares or robust regression can be used to address heteroscedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a303bb9",
   "metadata": {},
   "source": [
    "# Q19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536adbef",
   "metadata": {},
   "source": [
    "Multicollinearity in regression occurs when two or more independent variables are highly correlated, making it difficult to determine their individual effects on the dependent variable. It can lead to unstable coefficient estimates and inflated standard errors. Techniques to handle multicollinearity include removing or combining correlated variables, using dimensionality reduction methods, or employing regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c91448",
   "metadata": {},
   "source": [
    "# Q20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726430ed",
   "metadata": {},
   "source": [
    "Polynomial regression is a form of multiple linear regression where polynomial terms of the independent variables are included as predictors. It allows for modeling non-linear relationships between the variables by introducing additional polynomial terms (e.g., quadratic or cubic) to capture curvature or other complex patterns in the data. Polynomial regression is used when the relationship between the variables cannot be adequately captured by a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40405db8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
